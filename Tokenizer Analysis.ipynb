{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655f0b64",
   "metadata": {},
   "source": [
    "# **Analysing The Python Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e652c3",
   "metadata": {},
   "source": [
    "The tokenizer is built on top of GPT2's default tokenizer.\n",
    "Note: we are builing and deploying the tokenizer  to HF hub with `src/tokenizer.py`, I am just Analysing the performance in the following section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322446e",
   "metadata": {},
   "source": [
    "The tokenizer is using BPE algorithm that deals with Unicode strings. It maps the first 256 bytes to the unicode characters. There are many control charactrers, i.e. newline, tab, escape, line feed, and other nonprintable characters. The GPT2 Tokenizer maps the 256 elementary values to Unicode strings that all correspond to standard\n",
    "printable Unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fabbb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ee4e5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our base vocabulary: 256\n",
      "First element: `!`, last element: `Ń`\n"
     ]
    }
   ],
   "source": [
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "print(f'Size of our base vocabulary: {len(base_vocab)}')\n",
    "print(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "defebb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"razhan/codeqmul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8c61a",
   "metadata": {},
   "source": [
    "## Longest words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0977e29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n                                                                                                  ', '\\n                                                                                                ', '################################################################################################', '\\n                                                                                              ', '\\n                                                                                            ', '\\n                                                                                          ', '\\n                                                                                        ', '\\n                                                                                      ', '\\n                                                                                    ', '\\n                                                                                  ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:10]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e6152",
   "metadata": {},
   "source": [
    "That makes sense. As we can see it's either a long line of space or a long line of hash which is used for commenting code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5841d1",
   "metadata": {},
   "source": [
    "## Least common words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0ee1df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['658', ' uptime', \" '>',\", ' RPN', ' fiscal', 'TracedValue', 'Sale', 'Finds', 'MORE', 'fen', '%\",', 'correctly', 'Metaclass', ' Consumer', 'arena']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce8ea5",
   "metadata": {},
   "source": [
    "The last words added to the vocabulary are the least common occuring words in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b746a0",
   "metadata": {},
   "source": [
    "## First tokens after the first 256 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbca4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' p', 'ge', ' re', 'ur', '--', 'ce', ' \"', ' n', '):', 'mp', 'it', ' s', 'lo', 'ue', ' in', 'ame', 'ut', 'ing', ' o', 'ct', 'def', 'pe', 'ate', \"',\", '\\n                ', ' a', 'el', 'id', '\\n                  ', 'ser', '##', '\\n\\n   ', 'fi']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:290]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148e6af",
   "metadata": {},
   "source": [
    "If we skip the first 256 bytes of the vocabulary we can see various levels of indetation. This makes sense, since Python is indentation based programming language and we don't want to lose those becuase it's important for our model to generate correct programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57465a36",
   "metadata": {},
   "source": [
    "## The last words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c91f6670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arena', ' Consumer', 'Metaclass', 'correctly', '%\",', 'fen', 'MORE', 'Finds', 'Sale', 'TracedValue', ' fiscal', ' RPN', \" '>',\", ' uptime', '658']\n"
     ]
    }
   ],
   "source": [
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-15:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ede7a",
   "metadata": {},
   "source": [
    "We can see some operators and all the special tokens we added for begining of sentence, end of sentence, padded text, and finally unknown text. This shows our tokenizer works as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9b7a2",
   "metadata": {},
   "source": [
    "## Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b7c0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġis', '_', 'prime', '(', 'n', '):', 'ĊĠĠĠ', 'Ġfor', 'Ġi', 'Ġin', 'Ġrange', '(', '2', ',', 'int', '(', 'n', '**', '0', '.', '5', ')+', '1', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġif', 'Ġn', '%', 'i', '==', '0', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġreturn', 'ĠFalse', 'ĊĠĠĠ', 'Ġreturn', 'ĠTrue']\n"
     ]
    }
   ],
   "source": [
    "python_code =\"\"\"def is_prime(n):\n",
    "    for i in range(2,int(n**0.5)+1):\n",
    "        if n%i==0:\n",
    "            return False\n",
    "    return True\"\"\"\n",
    "\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42a0fe",
   "metadata": {},
   "source": [
    "It's working perfectly since the symbol indicates Ġ space and at the places where we have 4 or 8 spaces it treats the all the spaces as one token, even where we have a newline which is represented by Ċ caret symbol, it's treated as one token where it occurs frequently. This makes or tokenizer much more efficient since it does not have to treat each space separetly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594a034",
   "metadata": {},
   "source": [
    "## Checking if all the python reserved keywords are in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b417da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 35 reserved keywords in the python language.\n",
      "`nonlocal` is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(f'There are in total {len(keyword.kwlist)} reserved keywords in the python language.')\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in tokenizer.vocab:\n",
    "        print(f'`{keyw}` is not in the vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e54459",
   "metadata": {},
   "source": [
    "Only `nonlocal` is not the vocabulary. That's okay, since that word is not used frequently, therefore, not including in the vocab is not gonna effect the performance. I tried to give the tokenizer twice of a bigger protion of the dataset. It still did not contain the nonlocal keyword. The tokenizer is trained on 20% of the corpus which is a good representation of the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae8298",
   "metadata": {},
   "source": [
    "## Conclusion of the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44c8f4",
   "metadata": {},
   "source": [
    "In comparision our brand new tokenizer trained on the code corpus is at least twice as good as the original tokenizer provided by GPT-2. We can see the sequence length generated by our tokenizer is half of the length of the sequences generated by the default tokenizer. This will allow us to have double of the model context as before. In my case it will be 4 times model context as GPT-2 since I used GTP-Neo, the window size is increased to 2048 from 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25987220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
